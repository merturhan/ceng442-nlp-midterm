{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "dataset = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Emoji Cleaner Regex\n",
    "def remove_emojis(text):\n",
    "    if text is None:  # Check if the text is None\n",
    "        return None\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           u\"\\U0001f926-\\U0001f937\"\n",
    "                           u\"\\U00010000-\\U0010ffff\"\n",
    "                           u\"\\u2640-\\u2642\"\n",
    "                           u\"\\u2600-\\u2B55\"\n",
    "                           u\"\\u200d\"\n",
    "                           u\"\\u23cf\"\n",
    "                           u\"\\u23e9\"\n",
    "                           u\"\\u231a\"\n",
    "                           u\"\\ufe0f\"  # dingbats\n",
    "                           u\"\\u3030\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zğüşıöçəĞÜŞİÖÇƏ ]', '', text)  # Keep Azerbaijani Turkish letters and whitespace\n",
    "    return text if text.strip() != '' else None  # Return None if text is empty\n",
    "\n",
    "# Define the stopwords removal function\n",
    "def remove_stopwords(text):\n",
    "    if text is None:  # Check if the text is None\n",
    "        return None\n",
    "    words = text.split()\n",
    "    turkish_stopwords = stopwords.words('turkish')\n",
    "    filtered_text = ' '.join([word for word in words if word not in turkish_stopwords])\n",
    "    return filtered_text if filtered_text.strip() != '' else None  # Return None if filtered text is empty\n",
    "\n",
    "\n",
    "\n",
    "# Clear emojis and make all lowercased\n",
    "dataset['content'] = dataset['content'].apply(lambda x: remove_emojis(str(x)).lower())\n",
    "# Filter rows where 'content' column contains Turkish or Azerbaijani characters, \n",
    "dataset = dataset[dataset['content'].apply(lambda x: bool(clean_text(x)))]\n",
    "\n",
    "\n",
    "# Reset index after filtering\n",
    "dataset.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating contents, scores and upvotes\n",
    "contents = dataset['content'].values.tolist()\n",
    "scores = dataset['score'].values.tolist()\n",
    "upvotes = dataset['upvotes'].values.tolist()\n",
    "\n",
    "# Setting the style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Creating a figure with two subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "# Plotting histogram for 'score'\n",
    "axes[0].hist(scores, bins=range(1, 7), edgecolor='black', color='skyblue')\n",
    "axes[0].set_title('Distribution of Sentiment Scores')\n",
    "axes[0].set_xlabel('Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Plotting histogram for 'upvotes'\n",
    "axes[1].hist(upvotes, bins=range(1, 100), edgecolor='black', color='lightgreen')\n",
    "axes[1].set_title('Distribution of Upvotes')\n",
    "axes[1].set_xlabel('Upvotes')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "# Tight layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_upvotes_means = dataset.groupby('score')['upvotes'].mean()\n",
    "\n",
    "# Çubuk grafiği oluşturma\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=score_upvotes_means.index, y=score_upvotes_means.values, palette='viridis')\n",
    "plt.title('Average Upvotes by Sentiment Score')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Average Upvotes')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('cleaner', FunctionTransformer(lambda x: x.apply(clean_text))),\n",
    "    ('stopwords_remover', FunctionTransformer(lambda x: x.apply(remove_stopwords))),\n",
    "    ('cleaner_emojies', FunctionTransformer(lambda x: x.apply(remove_emojis)))  # Adding vectorizer to convert text into a numeric format\n",
    "])\n",
    "\n",
    "# Assuming 'dataset' is your DataFrame and it contains the 'content' column\n",
    "processed_data = pipeline.fit_transform(dataset['content'])\n",
    "\n",
    "# Filter out None values and split the data\n",
    "# 'processed_data' is a sparse matrix, need to convert dataset['score'] accordingly\n",
    "valid_indices = [i for i, text in enumerate(dataset['content']) if text is not None]\n",
    "X = processed_data[valid_indices]  # Filter the processed data\n",
    "y = dataset['score'].iloc[valid_indices]  # Filter the target variable accordingly\n",
    "\n",
    "# Splitting the dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_text_nltk(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    return word_tokenize(text, language='turkish')\n",
    "\n",
    "# Eğitim ve test verilerini tokenleme ve None değerleri filtreleme\n",
    "X_train_tokenized = X_train.apply(lambda x: tokenize_text_nltk(x) if x is not None else None)\n",
    "X_test_tokenized = X_test.apply(lambda x: tokenize_text_nltk(x) if x is not None else None)\n",
    "\n",
    "# None değerlerini filtreleyerek saf veri setlerini elde et\n",
    "X_train_filtered = X_train_tokenized[X_train_tokenized.notnull()]\n",
    "y_train_filtered = y_train[X_train_tokenized.notnull()]\n",
    "X_test_filtered = X_test_tokenized[X_test_tokenized.notnull()]\n",
    "y_test_filtered = y_test[X_test_tokenized.notnull()]\n",
    "\n",
    "# İlk birkaç tokenleşmiş örnekleri göster\n",
    "print(\"Tokenized Training Data Examples:\", X_train_filtered.head())\n",
    "print(\"Tokenized Test Data Examples:\", X_test_filtered.head())\n",
    "\n",
    "# Tokenizer'ı oluştur\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train_filtered)\n",
    "\n",
    "# Tokenizer'ı kullanarak eğitim ve test verilerini dönüştür\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train_filtered)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test_filtered)\n",
    "\n",
    "# Eğitim ve test verilerini sabit bir uzunluğa doldur\n",
    "X_train_padded = pad_sequences(X_train_sequences, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, padding='post', maxlen=X_train_padded.shape[1])\n",
    "\n",
    "# Tokenizer'ın kelime indekslerini ve kelime sayısını al\n",
    "word_index = tokenizer.word_index\n",
    "num_words = len(word_index) + 1\n",
    "\n",
    "\n",
    "\n",
    "# Implement models using pre-trained GloVe embeddings\n",
    "# Load the GloVe embeddings\n",
    "embeddings_index = {}\n",
    "with open('/Users/merturhan/Downloads/glove/glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# Create an embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, 100))\n",
    "for word, i in word_index.items():\n",
    "    if i < num_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential([\n",
    "    Embedding(num_words, 100, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "              input_length=X_train_padded.shape[1], trainable=False),\n",
    "    GRU(128, return_sequences=True),\n",
    "    GRU(128),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_padded, y_train_filtered, epochs=3, batch_size=32, validation_data=(X_test_padded, y_test_filtered))\n",
    "\n",
    "# Plot the training and validation accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yenidena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
