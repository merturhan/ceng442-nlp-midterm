{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your dataset into a pandas DataFrame\n",
    "# Assuming your dataset is in a CSV file named 'dataset.csv'\n",
    "dataset = pd.read_csv('train.csv')\n",
    "dataset = dataset.dropna(subset=['content'])\n",
    "\n",
    "glove_path = 'glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all the words\n",
    "dataset['content'] = dataset['content'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Emojis\n",
    "import re\n",
    "\n",
    "def remove_emojis(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                           u\"\\U00002500-\\U00002BEF\"\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           u\"\\U0001f926-\\U0001f937\"\n",
    "                           u\"\\U00010000-\\U0010ffff\"\n",
    "                           u\"\\u2640-\\u2642\"\n",
    "                           u\"\\u2600-\\u2B55\"\n",
    "                           u\"\\u200d\"\n",
    "                           u\"\\u23cf\"\n",
    "                           u\"\\u23e9\"\n",
    "                           u\"\\u231a\"\n",
    "                           u\"\\ufe0f\"  # dingbats\n",
    "                           u\"\\u3030\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/merturhan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Clean stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    words = text.split()\n",
    "    az_stopwords = stopwords.words('azerbaijani')\n",
    "    filtered_text = ' '.join([word for word in words if word not in az_stopwords])\n",
    "    return filtered_text if filtered_text.strip() != '' else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "import math\n",
    "\n",
    "def clean_text(text):\n",
    "    if text is None or isinstance(text, float) and math.isnan(text):\n",
    "        return None\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zğüşıöçəĞÜŞİÖÇƏ\\s]', ' ', text)  # Replace punctuations with whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\b\\d+\\b', ' ', text)\n",
    "    return text.strip() if text.strip() != '' else None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('cleaner', FunctionTransformer(lambda x: x.apply(clean_text))),\n",
    "    ('stopwords_remover', FunctionTransformer(lambda x: x.apply(remove_stopwords))),\n",
    "    ('cleaner_emojies', FunctionTransformer(lambda x: x.apply(remove_emojis))),\n",
    "    ('dropna', FunctionTransformer(lambda x: x.dropna().reset_index(drop=True)))\n",
    "])\n",
    "\n",
    "processed_data = pipeline.fit_transform(dataset['content']).dropna().reset_index(drop=True)\n",
    "\n",
    "dataset['content'] = processed_data\n",
    "dataset = dataset.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114934\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114934\n"
     ]
    }
   ],
   "source": [
    "def duplicate_rows(row):\n",
    "    return pd.concat([row] * row['upvotes'], ignore_index=True)\n",
    "\n",
    "print(len(dataset))\n",
    "\n",
    "repeated_indices = dataset.index.repeat(dataset['upvotes'] + 1) \n",
    "\n",
    "# Rows with more than 0 upvotes will be repeated 'upvotes' times\n",
    "dataset = dataset.loc[repeated_indices].reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1\n",
      "1     0\n",
      "2     1\n",
      "3     1\n",
      "4     1\n",
      "5     1\n",
      "6     1\n",
      "7     1\n",
      "8     1\n",
      "9     1\n",
      "10    0\n",
      "11    1\n",
      "12    1\n",
      "13    1\n",
      "14    0\n",
      "15    1\n",
      "16    1\n",
      "17    1\n",
      "18    1\n",
      "19    1\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 'score' sütununu 5'e bölelim\n",
    "dataset['sentiment'] = dataset['score'] / 5\n",
    "\n",
    "# Sentiment değerlerini güncelleyelim\n",
    "dataset['sentiment'] = dataset['sentiment'].apply(lambda x: 0 if x < 0.5 else 1)\n",
    "\n",
    "print(dataset['sentiment'][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = dataset['sentiment'].values.tolist()\n",
    "reviews = dataset['content'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "split_point = int(len(reviews) * 0.80)\n",
    "train_reviews, test_reviews = reviews[:split_point], reviews[split_point:]\n",
    "train_ratings, test_ratings = ratings[:split_point], ratings[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "num_words = 10000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'əla': 1,\n",
       " 'super': 2,\n",
       " 'cox': 3,\n",
       " 'ela': 4,\n",
       " 'gözəl': 5,\n",
       " 'salam': 6,\n",
       " 'i': 7,\n",
       " 'pis': 8,\n",
       " 'proqramdır': 9,\n",
       " 'praqram': 10,\n",
       " 'gozel': 11,\n",
       " 'edirəm': 12,\n",
       " 'proqram': 13,\n",
       " 'yaxsidi': 14,\n",
       " 'olmur': 15,\n",
       " 'əsəb': 16,\n",
       " 'oldum': 17,\n",
       " 'əladı': 18,\n",
       " 'gicdiyir': 19,\n",
       " 'sdemir': 20,\n",
       " 'kankiret': 21,\n",
       " 'yarata': 22,\n",
       " 'bilmirsuzse': 23,\n",
       " 'yaratmiyinda': 24,\n",
       " 'bəyəndim': 25,\n",
       " 'ama': 26,\n",
       " 'rahat': 27,\n",
       " 'pul': 28,\n",
       " 'yaxsi': 29,\n",
       " 'eladi': 30,\n",
       " 'əladır': 31,\n",
       " 'superdi': 32,\n",
       " 've': 33,\n",
       " 'deyir': 34,\n",
       " 'verir': 35,\n",
       " 'bank': 36,\n",
       " 'yoxdu': 37,\n",
       " 'ələ': 38,\n",
       " 'bilmirəm': 39,\n",
       " 'men': 40,\n",
       " 'edirem': 41,\n",
       " 'her': 42,\n",
       " 'proqramdi': 43,\n",
       " 'ulduz': 44,\n",
       " 'yaxşıdı': 45,\n",
       " 'kart': 46,\n",
       " 'olmasa': 47,\n",
       " 'tez': 48,\n",
       " 'proqramdı': 49,\n",
       " 'olsa': 50,\n",
       " 'ne': 51,\n",
       " 'edin': 52,\n",
       " 'daxil': 53,\n",
       " 'bəyədim': 54,\n",
       " 'yaxşıdır': 55,\n",
       " 'şey': 56,\n",
       " 'təşəkkürlər': 57,\n",
       " 'mükəmməl': 58,\n",
       " 'işləmir': 59,\n",
       " 'bilmirem': 60,\n",
       " 'allah': 61,\n",
       " 'qeydiyyatdan': 62,\n",
       " 'ola': 63,\n",
       " 'necə': 64,\n",
       " 'gəlmir': 65,\n",
       " 'normal': 66,\n",
       " 'nece': 67,\n",
       " 'internet': 68,\n",
       " 'edə': 69,\n",
       " 'ancaq': 70,\n",
       " 'burda': 71,\n",
       " 'niye': 72,\n",
       " 'olun': 73,\n",
       " 'gözel': 74,\n",
       " 'təşəkkür': 75,\n",
       " 'vermir': 76,\n",
       " 'tətbiq': 77,\n",
       " 'yeni': 78,\n",
       " 'm': 79,\n",
       " 'ədv': 80,\n",
       " 'ele': 81,\n",
       " 'xeta': 82,\n",
       " 'işləyir': 83,\n",
       " 'proqrama': 84,\n",
       " 'bele': 85,\n",
       " 'kodu': 86,\n",
       " 'gözəldi': 87,\n",
       " 'zəhmət': 88,\n",
       " 'wey': 89,\n",
       " 'manat': 90,\n",
       " 'proqramı': 91,\n",
       " 'di': 92,\n",
       " 'kredit': 93,\n",
       " 'kapital': 94,\n",
       " 'problem': 95,\n",
       " 'başqa': 96,\n",
       " 'azn': 97,\n",
       " 'hec': 98,\n",
       " 'könüllü': 99,\n",
       " 'əmələ': 100,\n",
       " 'tətbiqdir': 101,\n",
       " 'kod': 102,\n",
       " 'mənim': 103,\n",
       " 'tövsiyə': 104,\n",
       " 'birbank': 105,\n",
       " 'proqramdir': 106,\n",
       " 'play': 107,\n",
       " 'sag': 108,\n",
       " 'əlavə': 109,\n",
       " 'mb': 110,\n",
       " 'razıyam': 111,\n",
       " 'karta': 112,\n",
       " 'razi': 113,\n",
       " 'dəfə': 114,\n",
       " 'praqramdi': 115,\n",
       " 'bolt': 116,\n",
       " 'elaa': 117,\n",
       " 'son': 118,\n",
       " 'guncelleme': 119,\n",
       " 'eladir': 120,\n",
       " 'olardı': 121,\n",
       " 'nar': 122,\n",
       " 'cavab': 123,\n",
       " 'girirsen': 124,\n",
       " 'bilərəm': 125,\n",
       " 'beyendim': 126,\n",
       " 'tedbiqe': 127,\n",
       " 'gəldi': 128,\n",
       " 'verdi': 129,\n",
       " 'ucun': 130,\n",
       " 'xahiş': 131,\n",
       " 'pulsuz': 132,\n",
       " 'store': 133,\n",
       " 'giriş': 134,\n",
       " 'gelirsen': 135,\n",
       " 'məndə': 136,\n",
       " 'gün': 137,\n",
       " 'istifade': 138,\n",
       " 'iwleyir': 139,\n",
       " 'raziyam': 140,\n",
       " 'yenilenme': 141,\n",
       " 'bakcell': 142,\n",
       " 'de': 143,\n",
       " 'geri': 144,\n",
       " 'mene': 145,\n",
       " 'xəta': 146,\n",
       " 'mobil': 147,\n",
       " 'ödəniş': 148,\n",
       " 'menim': 149,\n",
       " 'vaxt': 150,\n",
       " 'yazır': 151,\n",
       " 'ede': 152,\n",
       " 'ok': 153,\n",
       " 'yüklədim': 154,\n",
       " 'razı': 155,\n",
       " 'azercell': 156,\n",
       " 'sms': 157,\n",
       " 'islemir': 158,\n",
       " 'edim': 159,\n",
       " 'gəlir': 160,\n",
       " 'taksi': 161,\n",
       " 'süper': 162,\n",
       " 'verem': 163,\n",
       " 'əlaa': 164,\n",
       " 'xetasi': 165,\n",
       " 'mende': 166,\n",
       " 'internete': 167,\n",
       " 'xoşuma': 168,\n",
       " 'sey': 169,\n",
       " 'qəşəng': 170,\n",
       " 'gb': 171,\n",
       " 'kartı': 172,\n",
       " 'gore': 173,\n",
       " 'gozlenilmeyen': 174,\n",
       " 'teztez': 175,\n",
       " 'baglanma': 176,\n",
       " 'olub': 177,\n",
       " 'gözəldir': 178,\n",
       " 'abb': 179,\n",
       " 'kömək': 180,\n",
       " 'kontur': 181,\n",
       " 'gec': 182,\n",
       " 'lerik': 183,\n",
       " 'açılmır': 184,\n",
       " 'sifariş': 185,\n",
       " 'tətbiqdi': 186,\n",
       " 'xaiş': 187,\n",
       " 'zehmet': 188,\n",
       " 'program': 189,\n",
       " 'möhtəşəm': 190,\n",
       " 'donur': 191,\n",
       " 'sağolun': 192,\n",
       " 'yenidən': 193,\n",
       " 'yaxwi': 194,\n",
       " 'lalafo': 195,\n",
       " 'hamıya': 196,\n",
       " 'berbad': 197,\n",
       " 'digər': 198,\n",
       " 'istəyirəm': 199,\n",
       " 'əvvəl': 200,\n",
       " 'yazıram': 201,\n",
       " 'bilerem': 202,\n",
       " 'sürücü': 203,\n",
       " 'defe': 204,\n",
       " 'sərfəli': 205,\n",
       " 'elan': 206,\n",
       " 'baş': 207,\n",
       " 'gelmir': 208,\n",
       " 'gozeldi': 209,\n",
       " 'nedi': 210,\n",
       " 'bərbad': 211,\n",
       " 'beledana': 212,\n",
       " 'pisdi': 213,\n",
       " 'tətbiqi': 214,\n",
       " 'bonus': 215,\n",
       " 'xidmət': 216,\n",
       " 'yazılır': 217,\n",
       " 'tetbiqdi': 218,\n",
       " 'sağ': 219,\n",
       " 'etmek': 220,\n",
       " 'əladi': 221,\n",
       " 'elaaa': 222,\n",
       " 'sizə': 223,\n",
       " 'praqramdı': 224,\n",
       " 'qəşəy': 225,\n",
       " 'gelir': 226,\n",
       " 'e': 227,\n",
       " 'qiymeti': 228,\n",
       " 'düzgün': 229,\n",
       " 'tam': 230,\n",
       " 'hələ': 231,\n",
       " 'superdir': 232,\n",
       " 'etdim': 233,\n",
       " 'tetbiq': 234,\n",
       " 'tesekkur': 235,\n",
       " 'gedir': 236,\n",
       " 'neçə': 237,\n",
       " 's': 238,\n",
       " 'yaxsı': 239,\n",
       " 'pox': 240,\n",
       " 'elaqe': 241,\n",
       " 'mesaj': 242,\n",
       " 'en': 243,\n",
       " 'arada': 244,\n",
       " 'kartdan': 245,\n",
       " 'yazir': 246,\n",
       " 'isteyirem': 247,\n",
       " 'proqramda': 248,\n",
       " 'qaldirib': 249,\n",
       " 'insafiniz': 250,\n",
       " 'pulu': 251,\n",
       " 'zor': 252,\n",
       " 'ucudur': 253,\n",
       " 'kəsə': 254,\n",
       " 'proqrami': 255,\n",
       " 'olardi': 256,\n",
       " 'verdim': 257,\n",
       " 'qosuldum': 258,\n",
       " 'good': 259,\n",
       " 'ləğv': 260,\n",
       " 'sözlə': 261,\n",
       " 'qeydiyyat': 262,\n",
       " 'superr': 263,\n",
       " 'göstərir': 264,\n",
       " 'mümkün': 265,\n",
       " 'yukledim': 266,\n",
       " 'söz': 267,\n",
       " 'f': 268,\n",
       " 'tap': 269,\n",
       " 'varsa': 270,\n",
       " 'yaxşi': 271,\n",
       " 'geldi': 272,\n",
       " 'sora': 273,\n",
       " 'düzəldin': 274,\n",
       " 'çıxır': 275,\n",
       " 'problemi': 276,\n",
       " 'artiq': 277,\n",
       " 'girmək': 278,\n",
       " 'ucuz': 279,\n",
       " 'olmaq': 280,\n",
       " 'hal': 281,\n",
       " 'ise': 282,\n",
       " 'filialinda': 283,\n",
       " 'açmır': 284,\n",
       " 'konullu': 285,\n",
       " 'hədiyyə': 286,\n",
       " 'supper': 287,\n",
       " 'keçmək': 288,\n",
       " 'sizdən': 289,\n",
       " 'edv': 290,\n",
       " 'qeydiyatdan': 291,\n",
       " 'yaziram': 292,\n",
       " 'karti': 293,\n",
       " 'əlaaa': 294,\n",
       " 'n': 295,\n",
       " 'bankin': 296,\n",
       " 'leo': 297,\n",
       " 'sistem': 298,\n",
       " 'elave': 299,\n",
       " 'kartla': 300,\n",
       " 'tesekkurler': 301,\n",
       " 'size': 302,\n",
       " 'sadece': 303,\n",
       " 'sevdim': 304,\n",
       " 'eyni': 305,\n",
       " 'uğurlar': 306,\n",
       " 'axı': 307,\n",
       " 'olanda': 308,\n",
       " 'proqramın': 309,\n",
       " 'bankın': 310,\n",
       " 'olunur': 311,\n",
       " 'reklam': 312,\n",
       " 'hələki': 313,\n",
       " 'superrr': 314,\n",
       " 'qesey': 315,\n",
       " 'nömrə': 316,\n",
       " 'burdan': 317,\n",
       " 'zordu': 318,\n",
       " 'sagolun': 319,\n",
       " 'acilmir': 320,\n",
       " 'aradan': 321,\n",
       " 'tək': 322,\n",
       " 'ile': 323,\n",
       " 'çarx': 324,\n",
       " 'r': 325,\n",
       " 'sildim': 326,\n",
       " 'bəyənirəm': 327,\n",
       " 'verin': 328,\n",
       " 'dı': 329,\n",
       " 'köməklik': 330,\n",
       " 'atır': 331,\n",
       " 'çıxıb': 332,\n",
       " 'yaxsidir': 333,\n",
       " 'olsaydı': 334,\n",
       " 'zibil': 335,\n",
       " 'girmir': 336,\n",
       " 'dahada': 337,\n",
       " 'banka': 338,\n",
       " 'vurmaq': 339,\n",
       " 'həll': 340,\n",
       " 'edirik': 341,\n",
       " 'balans': 342,\n",
       " 'xidmet': 343,\n",
       " 'gun': 344,\n",
       " 'ala': 345,\n",
       " 'etmir': 346,\n",
       " 'yene': 347,\n",
       " 'tetbiqdir': 348,\n",
       " 'etdiyim': 349,\n",
       " 'faydalı': 350,\n",
       " 'qiymət': 351,\n",
       " 'tətbiqə': 352,\n",
       " 'qeder': 353,\n",
       " 'yenilənmə': 354,\n",
       " 'keçə': 355,\n",
       " 'tapa': 356,\n",
       " 'server': 357,\n",
       " 'istəyir': 358,\n",
       " 'ndi': 359,\n",
       " 'asan': 360,\n",
       " 'versiya': 361,\n",
       " 'bilmədim': 362,\n",
       " 'şifrə': 363,\n",
       " 'məncə': 364,\n",
       " 'telefon': 365,\n",
       " 'güzel': 366,\n",
       " 'məlumat': 367,\n",
       " 'xidməti': 368,\n",
       " 'dan': 369,\n",
       " 'zeif': 370,\n",
       " 'rahatdır': 371,\n",
       " 'hamiya': 372,\n",
       " 'butun': 373,\n",
       " 'məsləhət': 374,\n",
       " 'yolu': 375,\n",
       " 'çoxdu': 376,\n",
       " 'hele': 377,\n",
       " 'surucu': 378,\n",
       " 'nəzərə': 379,\n",
       " 'adam': 380,\n",
       " 'mumkun': 381,\n",
       " 'iş': 382,\n",
       " 'heleki': 383,\n",
       " 'bomba': 384,\n",
       " 'yükləmək': 385,\n",
       " 'programdi': 386,\n",
       " 'gozeldir': 387,\n",
       " 'kece': 388,\n",
       " 'qaldim': 389,\n",
       " 'ödənişləri': 390,\n",
       " 'al': 391,\n",
       " 'beyenirem': 392,\n",
       " 'qəbul': 393,\n",
       " 'yeniden': 394,\n",
       " 'vaxtlar': 395,\n",
       " 'yaxwidi': 396,\n",
       " 'deyə': 397,\n",
       " 'zəng': 398,\n",
       " 'edib': 399,\n",
       " 'g': 400,\n",
       " 'isleyir': 401,\n",
       " 'birde': 402,\n",
       " 'nədi': 403,\n",
       " 'həqiqətən': 404,\n",
       " 'serfeli': 405,\n",
       " 'göstərmir': 406,\n",
       " 'guya': 407,\n",
       " 'komek': 408,\n",
       " 'əlaqə': 409,\n",
       " 'duzeldin': 410,\n",
       " 'qaldım': 411,\n",
       " 'evvel': 412,\n",
       " 'gozəl': 413,\n",
       " 'basqa': 414,\n",
       " 'tutulur': 415,\n",
       " 'alış': 416,\n",
       " 'zəif': 417,\n",
       " 'idare': 418,\n",
       " 'duz': 419,\n",
       " 'mukemmel': 420,\n",
       " 'oz': 421,\n",
       " 'sifaris': 422,\n",
       " 'unibank': 423,\n",
       " 'tövsiyyə': 424,\n",
       " 'promo': 425,\n",
       " 'yeniliklər': 426,\n",
       " 'xais': 427,\n",
       " 'qeyd': 428,\n",
       " 'yazilir': 429,\n",
       " 'hara': 430,\n",
       " 'əvvəlki': 431,\n",
       " 'almaq': 432,\n",
       " 'isdifade': 433,\n",
       " 'halda': 434,\n",
       " 'yüklənmir': 435,\n",
       " 'qr': 436,\n",
       " 'nfc': 437,\n",
       " 'terter': 438,\n",
       " 'yükləyirəm': 439,\n",
       " 'keçən': 440,\n",
       " 'hazırda': 441,\n",
       " 'çarxı': 442,\n",
       " 'bahadır': 443,\n",
       " 'iyi': 444,\n",
       " 'qiyməti': 445,\n",
       " 'kese': 446,\n",
       " 'bilirəm': 447,\n",
       " 'yuklenmir': 448,\n",
       " 'həmdə': 449,\n",
       " 'hesab': 450,\n",
       " 'şeyi': 451,\n",
       " 'lazim': 452,\n",
       " 'problemlər': 453,\n",
       " 'çok': 454,\n",
       " 'alınmır': 455,\n",
       " 'meslehet': 456,\n",
       " 'mobile': 457,\n",
       " 'zeng': 458,\n",
       " 'kimidi': 459,\n",
       " 'eladı': 460,\n",
       " 'gundu': 461,\n",
       " 'heçnə': 462,\n",
       " 'cixir': 463,\n",
       " 'gire': 464,\n",
       " 'əlaaaaa': 465,\n",
       " 'd': 466,\n",
       " 'bi': 467,\n",
       " 'içində': 468,\n",
       " 'paketi': 469,\n",
       " 'praqramı': 470,\n",
       " 'əldə': 471,\n",
       " 'prablem': 472,\n",
       " 'baku': 473,\n",
       " 'yada': 474,\n",
       " 'tutur': 475,\n",
       " 'girə': 476,\n",
       " 'sagolsun': 477,\n",
       " 'halal': 478,\n",
       " 'qəpik': 479,\n",
       " 'hem': 480,\n",
       " 'nomre': 481,\n",
       " 'məni': 482,\n",
       " 'rahatdi': 483,\n",
       " 'cür': 484,\n",
       " 'yuklemek': 485,\n",
       " 'tovsiye': 486,\n",
       " 'yerə': 487,\n",
       " 'dəyər': 488,\n",
       " 'aktiv': 489,\n",
       " 'gündü': 490,\n",
       " 'nedir': 491,\n",
       " 'xidmeti': 492,\n",
       " 'azersel': 493,\n",
       " 'ni': 494,\n",
       " 'perfect': 495,\n",
       " 'fin': 496,\n",
       " 'tətbiqidir': 497,\n",
       " 'olundu': 498,\n",
       " 'başa': 499,\n",
       " 'balansa': 500,\n",
       " 'bilmir': 501,\n",
       " 'berbat': 502,\n",
       " 'pisdir': 503,\n",
       " 'isdifadə': 504,\n",
       " 'qewey': 505,\n",
       " 'gətirdiyiniz': 506,\n",
       " 'yeri': 507,\n",
       " 'online': 508,\n",
       " 'emeliyyat': 509,\n",
       " 'proqramdan': 510,\n",
       " 'kodunu': 511,\n",
       " 'müştəri': 512,\n",
       " 'ulduzda': 513,\n",
       " 'əlaaaa': 514,\n",
       " 'olacaq': 515,\n",
       " 'silib': 516,\n",
       " 'yenədə': 517,\n",
       " 'kartın': 518,\n",
       " 'aça': 519,\n",
       " 'aparmaq': 520,\n",
       " 'belede': 521,\n",
       " 'demək': 522,\n",
       " 'deye': 523,\n",
       " 'telefonum': 524,\n",
       " 'versin': 525,\n",
       " 'proqramin': 526,\n",
       " 'tewekkurler': 527,\n",
       " 'birdə': 528,\n",
       " 'duzgun': 529,\n",
       " 'xosuma': 530,\n",
       " 'nternet': 531,\n",
       " 'telefona': 532,\n",
       " 'əməlli': 533,\n",
       " 'yükləmişəm': 534,\n",
       " 'hesaba': 535,\n",
       " 'özünüz': 536,\n",
       " 'orda': 537,\n",
       " 'lazım': 538,\n",
       " 'elaaaa': 539,\n",
       " 'meni': 540,\n",
       " 'ehtiyac': 541,\n",
       " 'yeniləmə': 542,\n",
       " 'rahatdı': 543,\n",
       " 'əladir': 544,\n",
       " 'saolun': 545,\n",
       " 'girmek': 546,\n",
       " 'bonuslar': 547,\n",
       " 'sərfəlidir': 548,\n",
       " 'nədir': 549,\n",
       " 'görürəm': 550,\n",
       " 'nömrəni': 551,\n",
       " 'praqrami': 552,\n",
       " 'aghkö': 553,\n",
       " 'endirim': 554,\n",
       " 'bilen': 555,\n",
       " 'tetbiqi': 556,\n",
       " 'şifrəni': 557,\n",
       " 'gündür': 558,\n",
       " 'bas': 559,\n",
       " 'umico': 560,\n",
       " 'yerde': 561,\n",
       " 'axi': 562,\n",
       " 'guzel': 563,\n",
       " 'davam': 564,\n",
       " 'giris': 565,\n",
       " 'şeydi': 566,\n",
       " 'cok': 567,\n",
       " 'servernen': 568,\n",
       " 'verirəm': 569,\n",
       " 'səbəb': 570,\n",
       " 'donmalar': 571,\n",
       " 'man': 572,\n",
       " 'saytdir': 573,\n",
       " 'yer': 574,\n",
       " 'edrem': 575,\n",
       " 'digital': 576,\n",
       " 'vasitə': 577,\n",
       " 'bura': 578,\n",
       " 'kartda': 579,\n",
       " 'is': 580,\n",
       " 'dostlar': 581,\n",
       " 'asanlaşır': 582,\n",
       " 'telefonda': 583,\n",
       " 'işlərim': 584,\n",
       " 'komeklik': 585,\n",
       " 'normaldı': 586,\n",
       " 'deyer': 587,\n",
       " 'rahatlaşır': 588,\n",
       " 'çatdırılma': 589,\n",
       " 'elanları': 590,\n",
       " 'narazıyam': 591,\n",
       " 'minnətdaram': 592,\n",
       " 'gosterir': 593,\n",
       " 'tətbiqdən': 594,\n",
       " 'nəyə': 595,\n",
       " 'sürücülər': 596,\n",
       " 'zayı': 597,\n",
       " 'qoyun': 598,\n",
       " 'yüksək': 599,\n",
       " 'filialı': 600,\n",
       " 'bankda': 601,\n",
       " 'hansı': 602,\n",
       " 'gah': 603,\n",
       " 'odenis': 604,\n",
       " 'hamı': 605,\n",
       " 'həftə': 606,\n",
       " 'əhsən': 607,\n",
       " 'vallah': 608,\n",
       " 'yi': 609,\n",
       " 'işə': 610,\n",
       " 'yola': 611,\n",
       " 'idarə': 612,\n",
       " 'gorurem': 613,\n",
       " 'şəbəkə': 614,\n",
       " 'teşekkür': 615,\n",
       " 'odenisleri': 616,\n",
       " 'veriş': 617,\n",
       " 'tətbiqdə': 618,\n",
       " 'superrrr': 619,\n",
       " 'lazımdı': 620,\n",
       " 'coox': 621,\n",
       " 'keşbek': 622,\n",
       " 'z': 623,\n",
       " 'yüklədikdə': 624,\n",
       " 'sizden': 625,\n",
       " 'praqramdır': 626,\n",
       " 'nömrəmi': 627,\n",
       " 'yol': 628,\n",
       " 'bildiriş': 629,\n",
       " 'borc': 630,\n",
       " 'adima': 631,\n",
       " 'iwlemir': 632,\n",
       " 'onda': 633,\n",
       " 'falan': 634,\n",
       " 'serfelidi': 635,\n",
       " 'internetin': 636,\n",
       " 'interneti': 637,\n",
       " 'hediyye': 638,\n",
       " 'dəyişir': 639,\n",
       " 'təminatın': 640,\n",
       " 'edirsinizmi': 641,\n",
       " 'barmaq': 642,\n",
       " 'limiti': 643,\n",
       " 'telefonu': 644,\n",
       " 'köhnə': 645,\n",
       " 'axır': 646,\n",
       " 'xoşum': 647,\n",
       " 'belədə': 648,\n",
       " 'qepik': 649,\n",
       " 'bilərsiz': 650,\n",
       " 'coxx': 651,\n",
       " 'qeseng': 652,\n",
       " 'helə': 653,\n",
       " 'bankdan': 654,\n",
       " 'sadə': 655,\n",
       " 'özünə': 656,\n",
       " 'ikinci': 657,\n",
       " 'vaxtı': 658,\n",
       " 'k': 659,\n",
       " 'filialında': 660,\n",
       " 'yenileme': 661,\n",
       " 'hersey': 662,\n",
       " 'day': 663,\n",
       " 'gediş': 664,\n",
       " 'wolt': 665,\n",
       " 'təzə': 666,\n",
       " 'bağlı': 667,\n",
       " 'tewekkur': 668,\n",
       " 'tələb': 669,\n",
       " 'düzəliş': 670,\n",
       " 'bilmrem': 671,\n",
       " 'gözeldi': 672,\n",
       " 'yariyir': 673,\n",
       " 'praqrama': 674,\n",
       " 'eləmək': 675,\n",
       " 'kartım': 676,\n",
       " 'herkese': 677,\n",
       " 'internetsiz': 678,\n",
       " 'çoox': 679,\n",
       " 'cooox': 680,\n",
       " 'əlverişli': 681,\n",
       " 'etmirəm': 682,\n",
       " 'qariwiqdi': 683,\n",
       " 'diger': 684,\n",
       " 'əl': 685,\n",
       " 'maşın': 686,\n",
       " 'yerdə': 687,\n",
       " 'sizdə': 688,\n",
       " 'sebebi': 689,\n",
       " 'reklamı': 690,\n",
       " 'yaxşidi': 691,\n",
       " 'qeydiyat': 692,\n",
       " 'serf': 693,\n",
       " 'hecne': 694,\n",
       " 'hemde': 695,\n",
       " 'cashback': 696,\n",
       " 'manata': 697,\n",
       " 'yenide': 698,\n",
       " 'funksiyasini': 699,\n",
       " 'dəyişməsin': 700,\n",
       " 'app': 701,\n",
       " 'etsəniz': 702,\n",
       " 'nəsə': 703,\n",
       " 'gəlib': 704,\n",
       " 'sehv': 705,\n",
       " 'vermirdi': 706,\n",
       " 'sürücüləri': 707,\n",
       " 'bankdi': 708,\n",
       " 'oda': 709,\n",
       " 'günlük': 710,\n",
       " 'xeyri': 711,\n",
       " 'mümkündür': 712,\n",
       " 'teqbiqdir': 713,\n",
       " 'musteriyem': 714,\n",
       " 'durdugum': 715,\n",
       " 'zaminliye': 716,\n",
       " 'işlemir': 717,\n",
       " 'günə': 718,\n",
       " 'min': 719,\n",
       " 'avans': 720,\n",
       " 'baxa': 721,\n",
       " 'ol': 722,\n",
       " 'kecmek': 723,\n",
       " 'izi': 724,\n",
       " 'oluram': 725,\n",
       " 'sürətli': 726,\n",
       " 'azərbaycan': 727,\n",
       " 't': 728,\n",
       " 'melumat': 729,\n",
       " 'edilmesi': 730,\n",
       " 'imtina': 731,\n",
       " 'bilirem': 732,\n",
       " 'yenilenmeyen': 733,\n",
       " 'esla': 734,\n",
       " 'yenilemesin': 735,\n",
       " 'limitlerin': 736,\n",
       " 'yigisdiriblar': 737,\n",
       " 'sayt': 738,\n",
       " 'aşağı': 739,\n",
       " 'verib': 740,\n",
       " 'yerine': 741,\n",
       " 'satmaq': 742,\n",
       " 'hərdən': 743,\n",
       " 'adami': 744,\n",
       " 'boş': 745,\n",
       " 'pin': 746,\n",
       " 'ugurlar': 747,\n",
       " 'ayda': 748,\n",
       " 'tetbiqe': 749,\n",
       " 'yazın': 750,\n",
       " 'programdı': 751,\n",
       " 'filiali': 752,\n",
       " 'tövsiye': 753,\n",
       " 'əhali': 754,\n",
       " 'aldım': 755,\n",
       " 'tədbiq': 756,\n",
       " 'nese': 757,\n",
       " 'nezere': 758,\n",
       " 'deq': 759,\n",
       " 'onlayn': 760,\n",
       " 'promokod': 761,\n",
       " 'səbəbi': 762,\n",
       " 'programdır': 763,\n",
       " 'düşür': 764,\n",
       " 'edirlər': 765,\n",
       " 'yukleyirem': 766,\n",
       " 'yuklemisem': 767,\n",
       " 'limitsiz': 768,\n",
       " 'ödənişi': 769,\n",
       " 'mi': 770,\n",
       " 'almisam': 771,\n",
       " 'pulum': 772,\n",
       " 'km': 773,\n",
       " 'tapmaq': 774,\n",
       " 'yere': 775,\n",
       " 'ugurlu': 776,\n",
       " 'qat': 777,\n",
       " 'ünvana': 778,\n",
       " 'biler': 779,\n",
       " 'gunde': 780,\n",
       " 'praqramdir': 781,\n",
       " 'vururam': 782,\n",
       " 'verilir': 783,\n",
       " 'qalan': 784,\n",
       " 'sağlıq': 785,\n",
       " 'lazımdır': 786,\n",
       " 'gostermir': 787,\n",
       " 'olduğunu': 788,\n",
       " 'əməyi': 789,\n",
       " 'el': 790,\n",
       " 'qayda': 791,\n",
       " 'alinmir': 792,\n",
       " 'texniki': 793,\n",
       " 'bezi': 794,\n",
       " 'sizi': 795,\n",
       " 'çatanda': 796,\n",
       " 'bonba': 797,\n",
       " 'göstərin': 798,\n",
       " 'supet': 799,\n",
       " 'getdi': 800,\n",
       " 'birazda': 801,\n",
       " 'edende': 802,\n",
       " 'serverle': 803,\n",
       " 'yerinə': 804,\n",
       " 'verirem': 805,\n",
       " 'edrəm': 806,\n",
       " 'gece': 807,\n",
       " 'mənfi': 808,\n",
       " 'barədə': 809,\n",
       " 'girəndə': 810,\n",
       " 'deaktiv': 811,\n",
       " 'birdən': 812,\n",
       " 'gül': 813,\n",
       " 'girirəm': 814,\n",
       " 'halbuki': 815,\n",
       " 'gelib': 816,\n",
       " 'isteyir': 817,\n",
       " 'gəlmədi': 818,\n",
       " 'format': 819,\n",
       " 'baxmaq': 820,\n",
       " 'sizde': 821,\n",
       " 'unvana': 822,\n",
       " 'xahis': 823,\n",
       " 'qanun': 824,\n",
       " 'şəkildə': 825,\n",
       " 'əllərinizə': 826,\n",
       " 'yazılıb': 827,\n",
       " 'birbanka': 828,\n",
       " 'yoxlamadan': 829,\n",
       " 'çalışıram': 830,\n",
       " 'qiymet': 831,\n",
       " 'məsələn': 832,\n",
       " 'legv': 833,\n",
       " 'hərşey': 834,\n",
       " 'hesabıma': 835,\n",
       " 'oyun': 836,\n",
       " 'lazimdi': 837,\n",
       " 'şikayət': 838,\n",
       " 'adamı': 839,\n",
       " 'blok': 840,\n",
       " 'qara': 841,\n",
       " 'programa': 842,\n",
       " 'hətta': 843,\n",
       " 'yaradırlar': 844,\n",
       " 'baha': 845,\n",
       " 'bilmək': 846,\n",
       " 'bununla': 847,\n",
       " 'sozle': 848,\n",
       " 'servis': 849,\n",
       " 'teze': 850,\n",
       " 'işlədirəm': 851,\n",
       " 'bakseldən': 852,\n",
       " 'əvvəllər': 853,\n",
       " 'gəlsə': 854,\n",
       " 'bloka': 855,\n",
       " 'sifarişi': 856,\n",
       " 'gecə': 857,\n",
       " 'mence': 858,\n",
       " 'su': 859,\n",
       " 'ümumi': 860,\n",
       " 'edi': 861,\n",
       " 'faizi': 862,\n",
       " 'olmadan': 863,\n",
       " 'müqayisədə': 864,\n",
       " 'adi': 865,\n",
       " 'şləmir': 866,\n",
       " 'paketləri': 867,\n",
       " 'hami': 868,\n",
       " 'nn': 869,\n",
       " 'gedib': 870,\n",
       " 'sikim': 871,\n",
       " 'bilərsiniz': 872,\n",
       " 'komissiya': 873,\n",
       " 'normaldi': 874,\n",
       " 'yazirki': 875,\n",
       " 'zibili': 876,\n",
       " 'üzrə': 877,\n",
       " 'şükür': 878,\n",
       " 'uzun': 879,\n",
       " 'yazirsan': 880,\n",
       " 'bahadı': 881,\n",
       " 'paylaşanda': 882,\n",
       " 'işlər': 883,\n",
       " 'olmayan': 884,\n",
       " 'yolda': 885,\n",
       " 'açanda': 886,\n",
       " 'supersiz': 887,\n",
       " 'ordan': 888,\n",
       " 'əlverişlidir': 889,\n",
       " 'boltun': 890,\n",
       " 'görmək': 891,\n",
       " 'very': 892,\n",
       " 'izah': 893,\n",
       " 'ulduza': 894,\n",
       " 'telefonumda': 895,\n",
       " 'böyük': 896,\n",
       " 'si': 897,\n",
       " 'bilməz': 898,\n",
       " 'köçürmə': 899,\n",
       " 'rəm': 900,\n",
       " 'ildi': 901,\n",
       " 'qiymətlər': 902,\n",
       " 'limit': 903,\n",
       " 'güncəlləmə': 904,\n",
       " 'bilersiz': 905,\n",
       " 'qiymətləri': 906,\n",
       " 'ora': 907,\n",
       " 'yükləyin': 908,\n",
       " 'bes': 909,\n",
       " 'yarayir': 910,\n",
       " 'deyirki': 911,\n",
       " 'dərəcədə': 912,\n",
       " 'million': 913,\n",
       " 'yükləyə': 914,\n",
       " 'ödəmək': 915,\n",
       " 'maaş': 916,\n",
       " 'dəfələrlə': 917,\n",
       " 'mq': 918,\n",
       " 'silindi': 919,\n",
       " 'minnetdaram': 920,\n",
       " 'azərbaycanda': 921,\n",
       " 'coxdu': 922,\n",
       " 'düşmür': 923,\n",
       " 'yoxlayın': 924,\n",
       " 'uzaq': 925,\n",
       " 'yenede': 926,\n",
       " 'carx': 927,\n",
       " 'zamanda': 928,\n",
       " 'etmişəm': 929,\n",
       " 'eladır': 930,\n",
       " 'qeseydi': 931,\n",
       " 'hell': 932,\n",
       " 'acmir': 933,\n",
       " 'cixib': 934,\n",
       " 'durduq': 935,\n",
       " 'bəs': 936,\n",
       " 'sebeb': 937,\n",
       " 'kohne': 938,\n",
       " 'tətbiqin': 939,\n",
       " 'bizi': 940,\n",
       " 'dusur': 941,\n",
       " 'zibildi': 942,\n",
       " 'xeyir': 943,\n",
       " 'edirdim': 944,\n",
       " 'qalır': 945,\n",
       " 'qəşəydi': 946,\n",
       " 'vaxtinda': 947,\n",
       " 'haqda': 948,\n",
       " 'zay': 949,\n",
       " 'onuda': 950,\n",
       " 'qeyri': 951,\n",
       " 'işdiyir': 952,\n",
       " 'yuklemir': 953,\n",
       " 'praqramıynan': 954,\n",
       " 'soz': 955,\n",
       " 'yoxa': 956,\n",
       " 'gündən': 957,\n",
       " 'can': 958,\n",
       " 'təkrar': 959,\n",
       " 'root': 960,\n",
       " 'kartıma': 961,\n",
       " 'sistemi': 962,\n",
       " 'like': 963,\n",
       " 'açılır': 964,\n",
       " 'balansı': 965,\n",
       " 'etmirem': 966,\n",
       " 'çəkə': 967,\n",
       " 'qeşeydi': 968,\n",
       " 'neye': 969,\n",
       " 'xayiş': 970,\n",
       " 'kartına': 971,\n",
       " 'olsaydi': 972,\n",
       " 'deqiq': 973,\n",
       " 'yazin': 974,\n",
       " 'yuklemeye': 975,\n",
       " 'superrrrrr': 976,\n",
       " 'rahatdir': 977,\n",
       " 'sağol': 978,\n",
       " 'he': 979,\n",
       " 'mohtesem': 980,\n",
       " 'yaxın': 981,\n",
       " 'demek': 982,\n",
       " 'yazmaq': 983,\n",
       " 'hərkəsə': 984,\n",
       " 'tədbiqə': 985,\n",
       " 'neden': 986,\n",
       " 'bankı': 987,\n",
       " 'terminal': 988,\n",
       " 'yarayır': 989,\n",
       " 'etsin': 990,\n",
       " 'çoxdur': 991,\n",
       " 'əıa': 992,\n",
       " 'yazdım': 993,\n",
       " 'den': 994,\n",
       " 'evvelki': 995,\n",
       " 'elanin': 996,\n",
       " 'uje': 997,\n",
       " 'keyfiyyətli': 998,\n",
       " 'nələr': 999,\n",
       " 'olunmur': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the word index\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data to sequences\n",
    "train_tokens = tokenizer.texts_to_sequences(train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "super\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "# Display tokenized data\n",
    "print(train_reviews[800])\n",
    "print(train_tokens[800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize test data\n",
    "test_tokens = tokenizer.texts_to_sequences(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of tokens: 3.7606986415742387\n",
      "Max number of tokens: 177\n",
      "Index of max tokens: 222932\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate and display token statistics\n",
    "num_tokens = [len(tokens) for tokens in train_tokens + test_tokens]\n",
    "num_tokens = np.array(num_tokens)\n",
    "print(\"Mean number of tokens:\", np.mean(num_tokens))\n",
    "print(\"Max number of tokens:\", np.max(num_tokens))\n",
    "print(\"Index of max tokens:\", np.argmax(num_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum number of tokens based on mean and standard deviation\n",
    "max_tokens = int(np.mean(num_tokens) + 2 * np.std(num_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences\n",
    "train_tokens_pad = pad_sequences(train_tokens, maxlen=max_tokens)\n",
    "test_tokens_pad = pad_sequences(test_tokens, maxlen=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tokens shape: (202175, 13)\n",
      "Test tokens shape: (50544, 13)\n"
     ]
    }
   ],
   "source": [
    "# Display padded data shapes\n",
    "print(\"Train tokens shape:\", train_tokens_pad.shape)\n",
    "print(\"Test tokens shape:\", test_tokens_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from index to word\n",
    "word_index = tokenizer.word_index\n",
    "inverse_map = dict(zip(word_index.values(), word_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to convert tokens back to text\n",
    "def tokens_to_string(tokens):\n",
    "    words = [inverse_map[token] for token in tokens if token != 0]\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "super\n",
      "super\n"
     ]
    }
   ],
   "source": [
    "# Display an example of converting tokens to text\n",
    "print(train_reviews[800])\n",
    "print(tokens_to_string(train_tokens[800]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the sequential model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set embedding size\n",
    "embedding_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add embedding layer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    name='embedding_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add GRU layers\n",
    "model.add(GRU(units=16, return_sequences=True))\n",
    "model.add(GRU(units=8, return_sequences=True))\n",
    "model.add(GRU(units=4))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                    │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                    │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                    │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_layer (\u001b[38;5;33mEmbedding\u001b[0m)     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_19 (\u001b[38;5;33mGRU\u001b[0m)                    │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_20 (\u001b[38;5;33mGRU\u001b[0m)                    │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_21 (\u001b[38;5;33mGRU\u001b[0m)                    │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m790/790\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 13ms/step - accuracy: 0.8117 - loss: 0.4750\n",
      "Epoch 2/10\n",
      "\u001b[1m790/790\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.8524 - loss: 0.3690\n",
      "Epoch 3/10\n",
      "\u001b[1m790/790\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.8617 - loss: 0.3414\n",
      "Epoch 4/10\n",
      "\u001b[1m790/790\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.8676 - loss: 0.3248\n",
      "Epoch 5/10\n",
      "\u001b[1m259/790\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.8721 - loss: 0.3103"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_ratings = np.array(train_ratings)\n",
    "model.fit(train_tokens_pad, train_ratings, epochs=10, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1580/1580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7788 - loss: 0.5417\n",
      "Test accuracy: 0.7791429162025452\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_ratings = np.array(test_ratings)\n",
    "evaluation_result = model.evaluate(test_tokens_pad, test_ratings)\n",
    "print(\"Test accuracy:\", evaluation_result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on a sample of data\n",
    "sample_predictions = model.predict(x=test_tokens_pad[0:1000]).T[0]\n",
    "predicted_classes = np.array([1.0 if p > 0.5 else 0.0 for p in sample_predictions])\n",
    "true_classes = np.array(test_ratings[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of incorrect predictions: 220\n"
     ]
    }
   ],
   "source": [
    "# Identify incorrect predictions\n",
    "incorrect_predictions = np.where(predicted_classes != true_classes)[0]\n",
    "print(\"Number of incorrect predictions:\", len(incorrect_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of incorrect prediction: 17\n",
      "Text: ela super\n",
      "Predicted Rating: 0.71103257\n",
      "True Rating: 0\n"
     ]
    }
   ],
   "source": [
    "# Display an example of incorrect prediction\n",
    "sample_index = incorrect_predictions[0]\n",
    "print(\"Index of incorrect prediction:\", sample_index)\n",
    "print(\"Text:\", test_reviews[sample_index])\n",
    "print(\"Predicted Rating:\", sample_predictions[sample_index])\n",
    "print(\"True Rating:\", true_classes[sample_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Predictions for new texts:\n",
      " [[0.9297277]]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on new sample texts\n",
    "new_texts = [\"Proqrama girmek olmur donub qalib bu ne meseledi?\"]\n",
    "new_tokens = tokenizer.texts_to_sequences(new_texts)\n",
    "new_tokens_pad = pad_sequences(new_tokens, maxlen=max_tokens)\n",
    "print(\"Predictions for new texts:\\n\", model.predict(new_tokens_pad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the GloVe embeddings\n",
    "embeddings_index = {}\n",
    "with open(glove_path, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# Create an embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, 300))\n",
    "for word, i in word_index.items():\n",
    "    if i < num_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "\n",
    "glove_model = Sequential([\n",
    "    Embedding(num_words, 300, \n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "              input_length=train_tokens_pad.shape[1], trainable=False),\n",
    "    GRU(units=16, return_sequences=True),\n",
    "    GRU(units=8, return_sequences=True),\n",
    "    GRU(units=4),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "glove_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "train_ratings = np.array(train_ratings)\n",
    "glove_model.fit(train_tokens_pad, train_ratings, epochs=5, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from gensim.models import KeyedVectors\n",
    " \n",
    "# Load Word2Vec embeddings\n",
    "model_path = 'GoogleNews-vectors-negative300.bin'\n",
    "word_vectors = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    " \n",
    "embedding_matrix = np.zeros((num_words, word_vectors.vector_size))\n",
    "for word, i in word_index.items():\n",
    "    if i < num_words:\n",
    "        if word in word_vectors:\n",
    "            embedding_matrix[i] = word_vectors[word]\n",
    "\n",
    " \n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Embedding(num_words, word_vectors.vector_size,\n",
    "              embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "              input_length=train_tokens_pad.shape[1], trainable=False),\n",
    "    GRU(units=16, return_sequences=True),\n",
    "    GRU(units=8, return_sequences=True),\n",
    "    GRU(units=4),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    " \n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "# Assuming train_tokens_pad and train_ratings are defined and properly shaped\n",
    "train_ratings = np.array(train_ratings)\n",
    "history = model.fit(train_tokens_pad, train_ratings, epochs=5, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yenidena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
