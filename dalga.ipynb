{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading dataset\n",
    "dataset = pd.read_csv('train.csv')\n",
    "\n",
    "# Dropping rows with missing content\n",
    "dataset = dataset.dropna(subset=['content'])\n",
    "\n",
    "glove_path = 'glove.6B.300d.txt'\n",
    "\n",
    "debuggerEnabled = False\n",
    "mertEnabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all the words\n",
    "dataset['content'] = dataset['content'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emoji remover regex\n",
    "import re\n",
    "\n",
    "def remove_emojis(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                           u\"\\U00002500-\\U00002BEF\"\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           u\"\\U0001f926-\\U0001f937\"\n",
    "                           u\"\\U00010000-\\U0010ffff\"\n",
    "                           u\"\\u2640-\\u2642\"\n",
    "                           u\"\\u2600-\\u2B55\"\n",
    "                           u\"\\u200d\"\n",
    "                           u\"\\u23cf\"\n",
    "                           u\"\\u23e9\"\n",
    "                           u\"\\u231a\"\n",
    "                           u\"\\ufe0f\"\n",
    "                           u\"\\u3030\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords Cleaner\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    words = text.split()\n",
    "    az_stopwords = stopwords.words('azerbaijani')\n",
    "    filtered_text = ' '.join([word for word in words if word not in az_stopwords])\n",
    "    return filtered_text if filtered_text.strip() != '' else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleaner\n",
    "''' \n",
    "    This function removes nones, links, non Turkish and Azerbaijani characters, number and punctuations\n",
    "'''\n",
    "import math\n",
    "\n",
    "def clean_text(text):\n",
    "    if text is None or isinstance(text, float) and math.isnan(text):\n",
    "        return None\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zğüşıöçəĞÜŞİÖÇƏ\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\b\\d+\\b', ' ', text)\n",
    "    return text.strip() if text.strip() != '' else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dataset length before cleaning\n",
    "if debuggerEnabled:\n",
    "    print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# This pipeline will clean the text, remove stopwords and emojis\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('cleaner', FunctionTransformer(lambda x: x.apply(clean_text))),\n",
    "    ('stopwords_remover', FunctionTransformer(lambda x: x.apply(remove_stopwords))),\n",
    "    ('cleaner_emojies', FunctionTransformer(lambda x: x.apply(remove_emojis))),\n",
    "    ('dropna', FunctionTransformer(lambda x: x.dropna().reset_index(drop=True)))\n",
    "])\n",
    "\n",
    "# Applying the pipeline to the dataset content column and dropping rows with missing content\n",
    "# Also resetting the index \n",
    "processed_data = pipeline.fit_transform(dataset['content']).dropna().reset_index(drop=True)\n",
    "\n",
    "# Assigning the processed data to the dataset\n",
    "dataset['content'] = processed_data\n",
    "dataset = dataset.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dataset length after cleaning\n",
    "if debuggerEnabled:\n",
    "    print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the score column by 5 to get the sentiment value\n",
    "dataset['sentiment'] = dataset['score'] / 5\n",
    "\n",
    "# Update the sentiment column to 0 if the value is less than 0.5, otherwise 1\n",
    "#dataset['sentiment'] = dataset['sentiment'].apply(lambda x: 0 if x < 0.5 else 1)\n",
    "\n",
    "# Show the first 20 rows of the dataset\n",
    "if debuggerEnabled:\n",
    "    print(dataset[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[66:77])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data distributions (e.g., sentiment scores, upvote distribution).\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plotting the sentiment distribution\n",
    "plt.hist(dataset['sentiment'])\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the upvotes distribution\n",
    "plt.hist(dataset['upvotes'])\n",
    "plt.xlabel('Upvotes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Upvotes Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the score distribution\n",
    "plt.hist(dataset['score'])\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Score Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><ul> \n",
    "Initial Findings\n",
    "\n",
    "<li> \n",
    "There were words that started with a capital letter so we lowercased all of the dataset\n",
    "</li>\n",
    "\n",
    "<li> \n",
    "We found unnecessary characters like emojis, non Turkish or Azerbaijani words, links, numbers, punctuations and nones in the dataset.\n",
    "</li>\n",
    "\n",
    "<li>\n",
    "There were some sentences that have punctuations between words without any whitespace\n",
    "</li>\n",
    "\n",
    "</ul></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate rows with upvotes more than 0\n",
    "def duplicate_rows(row):\n",
    "    return pd.concat([row] * row['upvotes'], ignore_index=True)\n",
    "\n",
    "# repeated_indices = dataset.index.repeat(dataset['upvotes'] + 1) \n",
    "\n",
    "# Assigning the repeated rows to the dataset\n",
    "# dataset = dataset.loc[repeated_indices].reset_index(drop=True) \n",
    "\n",
    "if debuggerEnabled:\n",
    "    print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into sentiment and content\n",
    "sentiment = dataset['sentiment'].values.tolist()\n",
    "content = dataset['content'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "split_point = int(len(content) * 0.80)\n",
    "train_content, test_content = content[:split_point], content[split_point:]\n",
    "train_sentiment, test_sentiment = sentiment[:split_point], sentiment[split_point:]\n",
    "\n",
    "train_dataset, test_dataset = dataset[:split_point], dataset[split_point:]\n",
    "\n",
    "print(len(train_content), len(test_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a sufficient amount of data in both the training and testing sets, statistical metrics such as accuracy, precision, recall, and F1 score computed on the testing set are more likely to provide meaningful insights into the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "num_words = 10000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(content)\n",
    "\n",
    "# Display the word index\n",
    "# tokenizer.word_index\n",
    "\n",
    "# Convert text data to sequences\n",
    "train_tokens = tokenizer.texts_to_sequences(train_content)\n",
    "\n",
    "# Display tokenized data\n",
    "if debuggerEnabled:\n",
    "    print(train_content[800])\n",
    "    print(train_tokens[800])\n",
    "\n",
    "# Tokenize test data\n",
    "test_tokens = tokenizer.texts_to_sequences(test_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization with NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def tokenize_text_nltk(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def tokenize_text_nltk_sentence(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    return sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization with Spacy\n",
    "import spacy\n",
    "import xx_sent_ud_sm\n",
    "\n",
    "# Must run in terminal \"python -m spacy download xx_sent_ud_sm\"\n",
    "\n",
    "spacy_data = spacy.load(\"xx_sent_ud_sm\")\n",
    "spacy_nlp = xx_sent_ud_sm.load()\n",
    "\n",
    "def tokenize_text_spacy(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    doc = spacy_nlp(text)\n",
    "    return [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mertEnabled:\n",
    "    # Tokenization with Polyglot\n",
    "    from polyglot.text import Text\n",
    "    def tokenize_text_polyglot(text, language_code='tr'):\n",
    "        if text is None:\n",
    "            return None\n",
    "        try:\n",
    "            polyglot_text = Text(text, hint_language_code=language_code)\n",
    "            return [token for token in polyglot_text.words]\n",
    "        except Exception as e:\n",
    "            print(f\"Error during tokenization: {e}\")\n",
    "            return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_operations(tokinization_name,train_content, test_content):\n",
    "    print(len(train_content))\n",
    "    if tokinization_name == 'spacy':\n",
    "        train_tokens = train_content.apply(lambda x: tokenize_text_spacy(x) if x is not None else None)\n",
    "        test_tokens = test_content.apply(lambda x: tokenize_text_spacy(x) if x is not None else None)\n",
    "    if tokinization_name == 'nltk':\n",
    "        train_tokens = train_content.apply(lambda x: tokenize_text_nltk(x))\n",
    "        test_tokens = test_content.apply(lambda x: tokenize_text_nltk(x))\n",
    "    if tokinization_name == 'polyglot':\n",
    "        train_tokens = train_content.apply(lambda x: tokenize_text_polyglot(x) if x is not None else None)\n",
    "        test_tokens = test_content.apply(lambda x: tokenize_text_polyglot(x) if x is not None else None)\n",
    "\n",
    "    if debuggerEnabled:\n",
    "        print(\"Tokenized Training Data Examples:\", train_tokens.head())\n",
    "        print(\"Tokenized Test Data Examples:\", test_tokens.head())\n",
    "        \n",
    "    return train_tokens, test_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data with NLTK\n",
    "train_token_nltk, test_token_nltk = tokenization_operations('nltk', train_dataset['content'], test_dataset['content'])\n",
    "print(len(train_token_nltk), len(test_token_nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data with Spacy\n",
    "test_token_spacy, train_token_spacy = tokenization_operations('spacy', train_dataset['content'], test_dataset['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data with Polyglot\n",
    "if mertEnabled:\n",
    "    test_token_polyglot, train_token_polyglot = tokenization_operations('polyglot', train_dataset['content'], test_dataset['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><ul> \n",
    "Comparison of Results\n",
    "\n",
    "<li> \n",
    "Fastest tokenizer was the NLTK\n",
    "</li>\n",
    "\n",
    "<li> \n",
    "We had some issues while installing polyglot on our computers\n",
    "</li\n",
    "\n",
    "</ul></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Flatten the list of lists\n",
    "flat_tokens = [token for sublist in train_token_nltk for token in sublist]\n",
    "\n",
    "# Convert token list to string\n",
    "text = ' '.join(flat_tokens)\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', font_path=None).generate(text)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('NLTK Tokenization Word Cloud')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of lists\n",
    "flat_tokens = [token for sublist in train_token_spacy for token in sublist]\n",
    "\n",
    "# Convert token list to string\n",
    "text = ' '.join(flat_tokens)\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', font_path=None).generate(text)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('SpaCy Tokenization Word Cloud')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of lists\n",
    "flat_tokens = [token for sublist in train_token_polyglot for token in sublist]\n",
    "\n",
    "# Convert token list to string\n",
    "text = ' '.join(flat_tokens)\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', font_path=None).generate(text)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Polyglot Tokenization Word Cloud')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate and display token statistics\n",
    "num_tokens = [len(tokens) for tokens in train_tokens + test_tokens]\n",
    "num_tokens = np.array(num_tokens)\n",
    "print(\"Mean number of tokens:\", np.mean(num_tokens))\n",
    "print(\"Max number of tokens:\", np.max(num_tokens))\n",
    "print(\"Index of max tokens:\", np.argmax(num_tokens))\n",
    "\n",
    "# Set the maximum number of tokens based on mean and standard deviation\n",
    "max_tokens = int(np.mean(num_tokens) + 2 * np.std(num_tokens))\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences\n",
    "train_tokens_pad = pad_sequences(train_tokens, maxlen=max_tokens)\n",
    "test_tokens_pad = pad_sequences(test_tokens, maxlen=max_tokens)\n",
    "print(train_tokens_pad.shape)\n",
    "print(test_tokens_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from index to word\n",
    "word_index = tokenizer.word_index\n",
    "inverse_map = dict(zip(word_index.values(), word_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to convert tokens back to text\n",
    "def tokens_to_string(tokens):\n",
    "    words = [inverse_map[token] for token in tokens if token != 0]\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an example of converting tokens to text\n",
    "print(train_content[800])\n",
    "print(tokens_to_string(train_tokens[800]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the sequential model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set embedding size\n",
    "embedding_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add embedding layer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "\n",
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    name='embedding_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add GRU layers\n",
    "model.add(GRU(units=16, return_sequences=True))\n",
    "model.add(GRU(units=8, return_sequences=True))\n",
    "model.add(GRU(units=4))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_sentiment = np.array(train_sentiment)\n",
    "trained_history = model.fit(train_tokens_pad, train_sentiment, epochs=10, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_sentiment = np.array(test_sentiment)\n",
    "evaluation_result = model.evaluate(test_tokens_pad, test_sentiment)\n",
    "print(\"Test accuracy:\", evaluation_result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on a sample of data\n",
    "sample_predictions = model.predict(x=test_tokens_pad[0:1000]).T[0]\n",
    "predicted_classes = np.array([1.0 if p > 0.5 else 0.0 for p in sample_predictions])\n",
    "true_classes = np.array(test_sentiment[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify incorrect predictions\n",
    "incorrect_predictions = np.where(predicted_classes != true_classes)[0]\n",
    "print(\"Number of incorrect predictions:\", len(incorrect_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an example of incorrect prediction\n",
    "sample_index = incorrect_predictions[66]\n",
    "print(\"Index of incorrect prediction:\", sample_index)\n",
    "print(\"Text:\", test_content[sample_index])\n",
    "print(\"Predicted Rating:\", sample_predictions[sample_index])\n",
    "print(\"True Rating:\", true_classes[sample_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on new sample texts\n",
    "new_texts = [\"Proqrama girmek olmur donub qalib bu ne meseledi?\"]\n",
    "new_tokens = tokenizer.texts_to_sequences(new_texts)\n",
    "new_tokens_pad = pad_sequences(new_tokens, maxlen=max_tokens)\n",
    "print(\"Predictions for new texts:\\n\", model.predict(new_tokens_pad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the GloVe embeddings\n",
    "embeddings_index = {}\n",
    "with open(glove_path, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# Create an embedding matrix\n",
    "embedding_matrix_glove = np.zeros((num_words, 300))\n",
    "for word, i in word_index.items():\n",
    "    if i < num_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix_glove[i] = embedding_vector\n",
    "\n",
    "glove_model = Sequential([\n",
    "    Embedding(num_words, 300, \n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix_glove),\n",
    "              input_length=train_tokens_pad.shape[1], trainable=False),\n",
    "    GRU(units=16, return_sequences=True),\n",
    "    GRU(units=8, return_sequences=True),\n",
    "    GRU(units=4),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "glove_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "train_sentiment = np.array(train_sentiment)\n",
    "history_glove=glove_model.fit(train_tokens_pad, train_sentiment, epochs=5, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from gensim.models import KeyedVectors\n",
    " \n",
    "# Load Word2Vec embeddings\n",
    "model_path = 'GoogleNews-vectors-negative300.bin'\n",
    "words = word_vectors.index_to_key\n",
    "\n",
    "embedding_matrix_w2v = np.zeros((num_words, word_vectors.vector_size))\n",
    "for word, i in word_index.items():\n",
    "    if i < num_words:\n",
    "        if word in word_vectors:\n",
    "            embedding_matrix_w2v[i] = word_vectors[word]\n",
    " \n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Embedding(num_words, word_vectors.vector_size,\n",
    "              embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix_w2v),\n",
    "              input_length=train_tokens_pad.shape[1], trainable=False),\n",
    "    GRU(units=16, return_sequences=True),\n",
    "    GRU(units=8, return_sequences=True),\n",
    "    GRU(units=4),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    " \n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "# Assuming train_tokens_pad and train_sentiment are defined and properly shaped\n",
    "train_sentiment = np.array(train_sentiment)\n",
    "history_w2v = model.fit(train_tokens_pad, train_sentiment, epochs=5, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import svd\n",
    "\n",
    "# Perform SVD\n",
    "U, S, VT = svd(embedding_matrix, full_matrices=False)\n",
    "\n",
    "# Select the number of components you want to keep\n",
    "num_features = min(50, len(words))\n",
    "U_reduced = U[:, :num_features]\n",
    "\n",
    "# Initialize a new embedding matrix with reduced dimensions\n",
    "embedding_matrix_SVD = np.zeros((len(words), num_features))\n",
    "print(num_features, \"features selected\")\n",
    "print(len(words))\n",
    "\n",
    "# Update the embedding matrix with reduced dimensions\n",
    "actual_num_words = min(len(words), U.shape[0])\n",
    "\n",
    "for i in range(actual_num_words):\n",
    "    embedding_matrix_SVD[i] = U_reduced[i]\n",
    "\n",
    "def train_generator():\n",
    "    for tokens, rating in zip(train_tokens_pad, train_sentiment):\n",
    "        yield tokens, rating\n",
    "\n",
    "# Assuming train_tokens_pad and train_sentiment are numpy arrays\n",
    "train_data = tf.data.Dataset.from_generator(\n",
    "    train_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.float32)\n",
    "    )\n",
    ").batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "history_svd=model.fit(train_data, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polyglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyglot.downloader import downloader\n",
    "downloader.download(\"embeddings2.az\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyglot.mapping import Embedding\n",
    "from tensorflow.keras.layers import Embedding as KerasEmbedding\n",
    "\n",
    "# Load Polyglot embeddings\n",
    "polyglot_embeddings = Embedding.load(\"./embeddings_pkl.tar.bz2\")\n",
    "\n",
    "# Create an embedding matrix\n",
    "embedding_matrix_polyglot = np.zeros((num_words, polyglot_embeddings.shape[1]))\n",
    "\n",
    "# Napping from word to integer index\n",
    "for word, i in word_index.items():\n",
    "    if i < num_words:\n",
    "        try:\n",
    "            embedding_vector = polyglot_embeddings[word]\n",
    "            embedding_matrix_polyglot[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    KerasEmbedding(input_dim=num_words, output_dim=polyglot_embeddings.shape[1],\n",
    "                   embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix_polyglot),\n",
    "                   input_length=train_tokens_pad.shape[1], trainable=False),\n",
    "    GRU(units=16, return_sequences=True),\n",
    "    GRU(units=8, return_sequences=True),\n",
    "    GRU(units=4),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "train_sentiment = np.array(train_sentiment)\n",
    "history_polyglot=model.fit(train_tokens_pad, train_sentiment, epochs=5, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FastText embeddings\n",
    "def load_fasttext_embeddings(embeddings_path):\n",
    "    embeddings_index = {}\n",
    "    with open(embeddings_path, 'r', encoding='utf-8') as f:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Path for FastText embeddings file\n",
    "embeddings_path = 'cc.az.300.vec'\n",
    "embeddings_index = load_fasttext_embeddings(embeddings_path)\n",
    "max_words=num_words\n",
    "\n",
    "# Mapping words to their integer indices and 'max_words'\n",
    "embedding_matrix_fastText = np.zeros((max_words, 300))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix_fastText[i] = embedding_vector\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Embedding(max_words, 300, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix_fastText), input_length=train_tokens_pad.shape[1], trainable=False),\n",
    "    GRU(units=16, return_sequences=True),\n",
    "    GRU(units=8),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Assume 'train_padded' is the padded sequence of training data and 'train_labels' are the labels\n",
    "# Train the model\n",
    "history_fastText = model.fit(train_tokens_pad, train_sentiment, epochs=5, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_models(history,name):\n",
    "    # Plotting training and validation accuracy\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.title('Accuracy over Epochs '+ name)\n",
    "    plt.legend()\n",
    "\n",
    "    # Plotting training and validation loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.title('Loss over Epochs '+ name)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "plot_models(trained_history,\"Developed First Model\")\n",
    "plot_models(history_fastText,\"FastText\")\n",
    "plot_models(history_glove,\"GloVE\")\n",
    "plot_models(history_w2v,\"Word2Vec\")\n",
    "plot_models(history_svd,\"SVD\")\n",
    "plot_models(history_polyglot,\"Polyglot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Example data\n",
    "df = pd.DataFrame(dataset['sentiment'])\n",
    "\n",
    "# Plotting the distribution of sentiment scores\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(df['sentiment'], bins=10, alpha=0.75, color='blue')\n",
    "plt.title('Distribution of Sentiment Scores')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Assuming 'embedding_matrix' is your matrix of word embeddings\n",
    "# Let's say it's shaped 10000x300 (10000 words, each with a 300-dimensional embedding)\n",
    "def embedding_space(embedding_matrix,name):\n",
    "# Using t-SNE to reduce dimensionality\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    reduced_embeddings = tsne.fit_transform(embedding_matrix[:1000])  # Using only the first 1000 for speed\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.5)\n",
    "    plt.title('Word Embeddings Space '+name)\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.show()\n",
    "embedding_space(embedding_matrix_glove, \"Glove\")\n",
    "embedding_space(embedding_matrix_fastText, \"FastText\")\n",
    "embedding_space(embedding_matrix_polyglot, \"Polyglot\")\n",
    "embedding_space(embedding_matrix_SVD,\"SVD\")\n",
    "embedding_space(embedding_matrix_w2v,\"Word2Vec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yenidena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
