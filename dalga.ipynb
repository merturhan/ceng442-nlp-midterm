{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your dataset into a pandas DataFrame\n",
    "# Assuming your dataset is in a CSV file named 'dataset.csv'\n",
    "dataset = pd.read_csv('train.csv')\n",
    "dataset = dataset.dropna(subset=['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all the words\n",
    "dataset['content'] = dataset['content'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Emojis\n",
    "import re\n",
    "\n",
    "def remove_emojis(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                           u\"\\U00002500-\\U00002BEF\"\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           u\"\\U0001f926-\\U0001f937\"\n",
    "                           u\"\\U00010000-\\U0010ffff\"\n",
    "                           u\"\\u2640-\\u2642\"\n",
    "                           u\"\\u2600-\\u2B55\"\n",
    "                           u\"\\u200d\"\n",
    "                           u\"\\u23cf\"\n",
    "                           u\"\\u23e9\"\n",
    "                           u\"\\u231a\"\n",
    "                           u\"\\ufe0f\"  # dingbats\n",
    "                           u\"\\u3030\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    words = text.split()\n",
    "    az_stopwords = stopwords.words('azerbaijani')\n",
    "    filtered_text = ' '.join([word for word in words if word not in az_stopwords])\n",
    "    return filtered_text if filtered_text.strip() != '' else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "import math\n",
    "\n",
    "def clean_text(text):\n",
    "    if text is None or isinstance(text, float) and math.isnan(text):\n",
    "        return None\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zğüşıöçəĞÜŞİÖÇƏ\\s]', ' ', text)  # Replace punctuations with whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\b\\d+\\b', ' ', text)\n",
    "    return text.strip() if text.strip() != '' else None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('cleaner', FunctionTransformer(lambda x: x.apply(clean_text))),\n",
    "    ('stopwords_remover', FunctionTransformer(lambda x: x.apply(remove_stopwords))),\n",
    "    ('cleaner_emojies', FunctionTransformer(lambda x: x.apply(remove_emojis))),\n",
    "    ('dropna', FunctionTransformer(lambda x: x.dropna().reset_index(drop=True)))\n",
    "])\n",
    "\n",
    "processed_data = pipeline.fit_transform(dataset['content']).dropna().reset_index(drop=True)\n",
    "\n",
    "dataset['content'] = processed_data\n",
    "dataset = dataset.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_rows(row):\n",
    "    return pd.concat([row] * row['upvotes'], ignore_index=True)\n",
    "\n",
    "print(len(dataset))\n",
    "\n",
    "repeated_indices = dataset.index.repeat(dataset['upvotes'] + 1) \n",
    "\n",
    "# Rows with more than 0 upvotes will be repeated 'upvotes' times\n",
    "dataset = dataset.loc[repeated_indices].reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'score' sütununu 5'e bölelim\n",
    "dataset['sentiment'] = dataset['score'] / 5\n",
    "\n",
    "# Sentiment değerlerini güncelleyelim\n",
    "dataset['sentiment'] = dataset['sentiment'].apply(lambda x: 0 if x < 0.5 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = dataset['sentiment'].values.tolist()\n",
    "reviews = dataset['content'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "split_point = int(len(reviews) * 0.80)\n",
    "train_reviews, test_reviews = reviews[:split_point], reviews[split_point:]\n",
    "train_ratings, test_ratings = ratings[:split_point], ratings[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "num_words = 10000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the word index\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data to sequences\n",
    "train_tokens = tokenizer.texts_to_sequences(train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display tokenized data\n",
    "print(train_reviews[800])\n",
    "print(train_tokens[800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize test data\n",
    "test_tokens = tokenizer.texts_to_sequences(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate and display token statistics\n",
    "num_tokens = [len(tokens) for tokens in train_tokens + test_tokens]\n",
    "num_tokens = np.array(num_tokens)\n",
    "print(\"Mean number of tokens:\", np.mean(num_tokens))\n",
    "print(\"Max number of tokens:\", np.max(num_tokens))\n",
    "print(\"Index of max tokens:\", np.argmax(num_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum number of tokens based on mean and standard deviation\n",
    "max_tokens = int(np.mean(num_tokens) + 2 * np.std(num_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences\n",
    "train_tokens_pad = pad_sequences(train_tokens, maxlen=max_tokens)\n",
    "test_tokens_pad = pad_sequences(test_tokens, maxlen=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display padded data shapes\n",
    "print(\"Train tokens shape:\", train_tokens_pad.shape)\n",
    "print(\"Test tokens shape:\", test_tokens_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from index to word\n",
    "word_index = tokenizer.word_index\n",
    "inverse_map = dict(zip(word_index.values(), word_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to convert tokens back to text\n",
    "def tokens_to_string(tokens):\n",
    "    words = [inverse_map[token] for token in tokens if token != 0]\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an example of converting tokens to text\n",
    "print(train_reviews[800])\n",
    "print(tokens_to_string(train_tokens[800]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the sequential model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set embedding size\n",
    "embedding_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add embedding layer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    name='embedding_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add GRU layers\n",
    "model.add(GRU(units=16, return_sequences=True))\n",
    "model.add(GRU(units=8, return_sequences=True))\n",
    "model.add(GRU(units=4))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_ratings = np.array(train_ratings)\n",
    "model.fit(train_tokens_pad, train_ratings, epochs=10, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yenidena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
