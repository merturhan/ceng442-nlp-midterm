{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca839c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from polyglot.text import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b93182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "spacy_data = spacy.load(\"xx_sent_ud_sm\")\n",
    "import xx_sent_ud_sm\n",
    "spacy_nlp = xx_sent_ud_sm.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff997eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "dataset = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa923ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17432e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Emoji Cleaner Regex\n",
    "def remove_emojis(text):\n",
    "    if text is None:  # Check if the text is None\n",
    "        return None\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           u\"\\U0001f926-\\U0001f937\"\n",
    "                           u\"\\U00010000-\\U0010ffff\"\n",
    "                           u\"\\u2640-\\u2642\"\n",
    "                           u\"\\u2600-\\u2B55\"\n",
    "                           u\"\\u200d\"\n",
    "                           u\"\\u23cf\"\n",
    "                           u\"\\u23e9\"\n",
    "                           u\"\\u231a\"\n",
    "                           u\"\\ufe0f\"  # dingbats\n",
    "                           u\"\\u3030\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zğüşıöçəĞÜŞİÖÇƏ ]', '', text)  # Keep Azerbaijani Turkish letters and whitespace\n",
    "    return text if text.strip() != '' else None  # Return None if text is empty\n",
    "\n",
    "# Define the stopwords removal function\n",
    "def remove_stopwords(text):\n",
    "    if text is None:  # Check if the text is None\n",
    "        return None\n",
    "    words = text.split()\n",
    "    turkish_stopwords = stopwords.words('turkish')\n",
    "    filtered_text = ' '.join([word for word in words if word not in turkish_stopwords])\n",
    "    return filtered_text if filtered_text.strip() != '' else None  # Return None if filtered text is empty\n",
    "\n",
    "\n",
    "\n",
    "# Clear emojis and make all lowercased\n",
    "dataset['content'] = dataset['content'].apply(lambda x: remove_emojis(str(x)).lower())\n",
    "# Filter rows where 'content' column contains Turkish or Azerbaijani characters, \n",
    "dataset = dataset[dataset['content'].apply(lambda x: bool(clean_text(x)))]\n",
    "\n",
    "\n",
    "# Reset index after filtering\n",
    "dataset.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eac8fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating contents, scores and upvotes\n",
    "contents = dataset['content'].values.tolist()\n",
    "scores = dataset['score'].values.tolist()\n",
    "upvotes = dataset['upvotes'].values.tolist()\n",
    "\n",
    "# Setting the style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Creating a figure with two subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "# Plotting histogram for 'score'\n",
    "axes[0].hist(scores, bins=range(1, 7), edgecolor='black', color='skyblue')\n",
    "axes[0].set_title('Distribution of Sentiment Scores')\n",
    "axes[0].set_xlabel('Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Plotting histogram for 'upvotes'\n",
    "axes[1].hist(upvotes, bins=range(1, 100), edgecolor='black', color='lightgreen')\n",
    "axes[1].set_title('Distribution of Upvotes')\n",
    "axes[1].set_xlabel('Upvotes')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "# Tight layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_upvotes_means = dataset.groupby('score')['upvotes'].mean()\n",
    "\n",
    "# Çubuk grafiği oluşturma\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=score_upvotes_means.index, y=score_upvotes_means.values, palette='viridis')\n",
    "plt.title('Average Upvotes by Sentiment Score')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Average Upvotes')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155cdeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('cleaner', FunctionTransformer(lambda x: x.apply(clean_text))),\n",
    "    ('stopwords_remover', FunctionTransformer(lambda x: x.apply(remove_stopwords))),\n",
    "    ('cleaner_emojies', FunctionTransformer(lambda x: x.apply(remove_emojis)))  # Adding vectorizer to convert text into a numeric format\n",
    "])\n",
    "\n",
    "# Assuming 'dataset' is your DataFrame and it contains the 'content' column\n",
    "processed_data = pipeline.fit_transform(dataset['content'])\n",
    "\n",
    "# Filter out None values and split the data\n",
    "# 'processed_data' is a sparse matrix, need to convert dataset['score'] accordingly\n",
    "valid_indices = [i for i, text in enumerate(dataset['content']) if text is not None]\n",
    "X = processed_data[valid_indices]  # Filter the processed data\n",
    "y = dataset['score'].iloc[valid_indices]  # Filter the target variable accordingly\n",
    "\n",
    "# Splitting the dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1298cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text_spacy(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    doc = spacy_nlp(text)\n",
    "    return [token.text for token in doc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a1e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text_nltk(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    return word_tokenize(text, language='turkish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3b47cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text_polyglot(text, language_code='tr'):\n",
    "    if text is None:\n",
    "        return None\n",
    "    try:\n",
    "        polyglot_text = Text(text, hint_language_code=language_code)\n",
    "        return [token for token in polyglot_text.words]\n",
    "    except Exception as e:\n",
    "        print(f\"Error during tokenization: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1c4bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_operations(tokinization_name,X_train, X_test, y_train, y_test):\n",
    "    # Eğitim ve test verilerini tokenleme ve None değerleri filtreleme\n",
    "    if tokinization_name == 'spacy':\n",
    "        X_train_tokenized = X_train.apply(lambda x: tokenize_text_spacy(x) if x is not None else None)\n",
    "        X_test_tokenized = X_test.apply(lambda x: tokenize_text_spacy(x) if x is not None else None)\n",
    "    if tokinization_name == 'nltk':\n",
    "        X_train_tokenized = X_train.apply(lambda x: tokenize_text_nltk(x) if x is not None else None)\n",
    "        X_test_tokenized = X_test.apply(lambda x: tokenize_text_nltk(x) if x is not None else None)\n",
    "    if tokinization_name == 'polyglot':\n",
    "        X_train_tokenized = X_train.apply(lambda x: tokenize_text_polyglot(x) if x is not None else None)\n",
    "        X_test_tokenized = X_test.apply(lambda x: tokenize_text_polyglot(x) if x is not None else None)\n",
    "    # None değerlerini filtreleyerek saf veri setlerini elde et\n",
    "    X_train_filtered = X_train_tokenized[X_train_tokenized.notnull()]\n",
    "    y_train_filtered = y_train[X_train_tokenized.notnull()]\n",
    "    X_test_filtered = X_test_tokenized[X_test_tokenized.notnull()]\n",
    "    y_test_filtered = y_test[X_test_tokenized.notnull()]\n",
    "\n",
    "    # İlk birkaç tokenleşmiş örnekleri göster\n",
    "    print(\"Tokenized Training Data Examples:\", X_train_filtered.head())\n",
    "    print(\"Tokenized Test Data Examples:\", X_test_filtered.head())\n",
    "    return X_train_filtered, X_test_filtered, y_train_filtered, y_test_filtered\n",
    "\n",
    "X_train_filtered_spacy, X_test_filtered_spacy, y_train_filtered_spacy, y_test_filtered_spacy=tokenization_operations('spacy',X_train, X_test, y_train, y_test)\n",
    "X_train_filtered_nltk, X_test_filtered_nltk, y_train_filtered_nltk, y_test_filtered_nltk=tokenization_operations('nltk',X_train, X_test, y_train, y_test)\n",
    "X_train_filtered_polyglot, X_test_filtered_polyglot, y_train_filtered_polyglot, y_test_filtered_polyglot=tokenization_operations('polyglot',X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00895d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model_with_gru(X_train_filtered, y_train_filtered, X_test_filtered, y_test_filtered):\n",
    "    # Tokenizer'ı oluştur\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_train_filtered)\n",
    "\n",
    "    # Tokenizer'ı kullanarak eğitim ve test verilerini dönüştür\n",
    "    X_train_sequences = tokenizer.texts_to_sequences(X_train_filtered)\n",
    "    X_test_sequences = tokenizer.texts_to_sequences(X_test_filtered)\n",
    "\n",
    "    # Eğitim ve test verilerini sabit bir uzunluğa doldur\n",
    "    X_train_padded = pad_sequences(X_train_sequences, padding='post')\n",
    "    X_test_padded = pad_sequences(X_test_sequences, padding='post', maxlen=X_train_padded.shape[1])\n",
    "\n",
    "    # Tokenizer'ın kelime indekslerini ve kelime sayısını al\n",
    "    word_index = tokenizer.word_index\n",
    "    num_words = len(word_index) + 1\n",
    "\n",
    "    # Eğitim ve test verilerini ve etiketlerini TensorFlow veri setlerine dönüştür\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train_padded, y_train_filtered))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((X_test_padded, y_test_filtered))\n",
    "\n",
    "    # Batch ve karıştırma işlemlerini uygula\n",
    "    batch_size = 256\n",
    "    train_dataset = train_dataset.shuffle(len(X_train_padded)).batch(batch_size)\n",
    "    test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "    # Modeli oluştur\n",
    "    model = Sequential([\n",
    "        Embedding(num_words, 32),\n",
    "        GRU(32),\n",
    "        Dense(6, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Modeli derle\n",
    "    model.compile(optimizer=Adam(1e-3),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # Modeli eğit\n",
    "    history = model.fit(train_dataset, epochs=5, validation_data=test_dataset)\n",
    "\n",
    "    # Eğitim ve doğrulama kayıplarını ve doğruluklarını çiz\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "baseline_model_with_gru(X_train_filtered_spacy, y_train_filtered_spacy, X_test_filtered_spacy, y_test_filtered_spacy)\n",
    "baseline_model_with_gru(X_train_filtered_nltk, y_train_filtered_nltk, X_test_filtered_nltk, y_test_filtered_nltk)\n",
    "baseline_model_with_gru(X_train_filtered_polyglot, y_train_filtered_polyglot, X_test_filtered_polyglot, y_test_filtered_polyglot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
