{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca839c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b93182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "spacy_data = spacy.load(\"xx_sent_ud_sm\")\n",
    "import xx_sent_ud_sm\n",
    "spacy_nlp = xx_sent_ud_sm.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff997eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "dataset = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa923ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17432e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Emoji Cleaner Regex\n",
    "def remove_emojis(text):\n",
    "    if text is None:  # Check if the text is None\n",
    "        return None\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           u\"\\U0001f926-\\U0001f937\"\n",
    "                           u\"\\U00010000-\\U0010ffff\"\n",
    "                           u\"\\u2640-\\u2642\"\n",
    "                           u\"\\u2600-\\u2B55\"\n",
    "                           u\"\\u200d\"\n",
    "                           u\"\\u23cf\"\n",
    "                           u\"\\u23e9\"\n",
    "                           u\"\\u231a\"\n",
    "                           u\"\\ufe0f\"  # dingbats\n",
    "                           u\"\\u3030\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zğüşıöçəĞÜŞİÖÇƏ ]', '', text)  # Keep Azerbaijani Turkish letters and whitespace\n",
    "    return text if text.strip() != '' else None  # Return None if text is empty\n",
    "\n",
    "# Define the stopwords removal function\n",
    "def remove_stopwords(text):\n",
    "    if text is None:  # Check if the text is None\n",
    "        return None\n",
    "    words = text.split()\n",
    "    turkish_stopwords = stopwords.words('turkish')\n",
    "    filtered_text = ' '.join([word for word in words if word not in turkish_stopwords])\n",
    "    return filtered_text if filtered_text.strip() != '' else None  # Return None if filtered text is empty\n",
    "\n",
    "\n",
    "\n",
    "# Clear emojis and make all lowercased\n",
    "dataset['content'] = dataset['content'].apply(lambda x: remove_emojis(str(x)).lower())\n",
    "# Filter rows where 'content' column contains Turkish or Azerbaijani characters, \n",
    "dataset = dataset[dataset['content'].apply(lambda x: bool(clean_text(x)))]\n",
    "\n",
    "\n",
    "# Reset index after filtering\n",
    "dataset.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eac8fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating contents, scores and upvotes\n",
    "contents = dataset['content'].values.tolist()\n",
    "scores = dataset['score'].values.tolist()\n",
    "upvotes = dataset['upvotes'].values.tolist()\n",
    "\n",
    "# Setting the style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Creating a figure with two subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "# Plotting histogram for 'score'\n",
    "axes[0].hist(scores, bins=range(1, 7), edgecolor='black', color='skyblue')\n",
    "axes[0].set_title('Distribution of Sentiment Scores')\n",
    "axes[0].set_xlabel('Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Plotting histogram for 'upvotes'\n",
    "axes[1].hist(upvotes, bins=range(1, 100), edgecolor='black', color='lightgreen')\n",
    "axes[1].set_title('Distribution of Upvotes')\n",
    "axes[1].set_xlabel('Upvotes')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "# Tight layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_upvotes_means = dataset.groupby('score')['upvotes'].mean()\n",
    "\n",
    "# Çubuk grafiği oluşturma\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=score_upvotes_means.index, y=score_upvotes_means.values, palette='viridis')\n",
    "plt.title('Average Upvotes by Sentiment Score')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Average Upvotes')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155cdeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('cleaner', FunctionTransformer(lambda x: x.apply(clean_text))),\n",
    "    ('stopwords_remover', FunctionTransformer(lambda x: x.apply(remove_stopwords))),\n",
    "    ('cleaner_emojies', FunctionTransformer(lambda x: x.apply(remove_emojis)))  # Adding vectorizer to convert text into a numeric format\n",
    "])\n",
    "\n",
    "# Assuming 'dataset' is your DataFrame and it contains the 'content' column\n",
    "processed_data = pipeline.fit_transform(dataset['content'])\n",
    "\n",
    "# Filter out None values and split the data\n",
    "# 'processed_data' is a sparse matrix, need to convert dataset['score'] accordingly\n",
    "valid_indices = [i for i, text in enumerate(dataset['content']) if text is not None]\n",
    "X = processed_data[valid_indices]  # Filter the processed data\n",
    "y = dataset['score'].iloc[valid_indices]  # Filter the target variable accordingly\n",
    "\n",
    "# Splitting the dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1298cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text_spacy(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    doc = spacy_nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "# Eğitim ve test verilerini tokenleme ve None değerleri filtreleme\n",
    "X_train_tokenized = X_train.apply(lambda x: tokenize_text_spacy(x) if x is not None else None)\n",
    "X_test_tokenized = X_test.apply(lambda x: tokenize_text_spacy(x) if x is not None else None)\n",
    "\n",
    "# None değerlerini filtreleyerek saf veri setlerini elde et\n",
    "X_train_filtered = X_train_tokenized[X_train_tokenized.notnull()]\n",
    "y_train_filtered = y_train[X_train_tokenized.notnull()]\n",
    "X_test_filtered = X_test_tokenized[X_test_tokenized.notnull()]\n",
    "y_test_filtered = y_test[X_test_tokenized.notnull()]\n",
    "\n",
    "# İlk birkaç tokenleşmiş örnekleri göster\n",
    "print(\"Tokenized Training Data Examples:\", X_train_filtered.head())\n",
    "print(\"Tokenized Test Data Examples:\", X_test_filtered.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
